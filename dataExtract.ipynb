{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purposes of this notebook is to use PySpark to extract data from parquet files as external tables, and then convert those external tables into csv for analysis in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate PySpark in the terminal\n",
    "! docker-compose exec spark env\n",
    "! PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 5000 --ip 0.0.0.0 --allow-root' pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above configuration will look different. We had to reconfigure our GCP to allow external traffic through port 5000. This was the only way we could connect with a PySpark notebook, hosted on GCP, with our local browsers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, start creating external tables from the written parquet files, locations include: \n",
    "\n",
    "-/tmp/play_hrs\n",
    "-/tmp/purchase_sword\n",
    "-/tmp/ref_count\n",
    "-/tmp/sub_count\n",
    "-/tmp/money_paid\n",
    "-/tmp/account_open\n",
    "-/tmp/join_guild\n",
    "-/tmp/sw_a_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 1, we need hours played and account_open\n",
    "\n",
    "#create temp tables for both variables\n",
    "hours = spark.read.parquet('/tmp/play_hrs')\n",
    "hours.registerTempTable('hours')\n",
    "acct_open = spark.read.parquet('/tmp/account_open')\n",
    "acct_open.registerTempTable('open')\n",
    "\n",
    "#now join tables on Player ID\n",
    "\n",
    "hrs_open = spark.sql(\n",
    "    \"select * from hrs_open \\\n",
    "    from hours \\\n",
    "    inner join acct_open \\\n",
    "    on hours.host = open.host\"\n",
    ")\n",
    "\n",
    "#export to df for analysis\n",
    "df_q1 = hrs_open.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 2, need friend referrals and to join with play time\n",
    "\n",
    "#create temp table \n",
    "ref = spark.read.parquet('/tmp/ref_count')\n",
    "ref.registerTempTable('ref')\n",
    "\n",
    "#inner join\n",
    "hrs_op_ref = spark.sql(\n",
    "    \"select * from hours_open \\\n",
    "    inner join ref \\\n",
    "    on hours_open.host = ref.host\"\n",
    ")\n",
    "\n",
    "#export to df for analysis\n",
    "df_q2 = hrs_op_ref.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query 3, need money and subscriptions, will join to hrs_op_ref in case\n",
    "\n",
    "#create temp tables\n",
    "sub = spark.read.parquet('/tmp/sub_count')\n",
    "sub.registerTempTable('sub')\n",
    "money = spark.read.parquet('/tmp/money_paid')\n",
    "money.registerTempTable('money')\n",
    "\n",
    "#join all 3 tables\n",
    "hrs_op_ref_mon_sub = spark.sql(\n",
    "    \"select * \\\n",
    "    from ((hrs_op_ref \\\n",
    "    inner join sub on hrs_op_ref.host = sub.host) \\\n",
    "    inner join money on hrs_op_ref.host = money.host)\"\n",
    ")\n",
    "\n",
    "#export to df for analysis\n",
    "df_q3 = hrs_op_ref_mon_sub.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
